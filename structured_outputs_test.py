#!/usr/bin/env python3
"""
Structured Outputs LLM API Request Script
Makes structured output requests to a local LLM API with configurable concurrency.
Calculates TTFT (Time To First Token) and TPOT (Tokens Per Output Token) metrics.
"""

import os
import json
import time
import argparse
import asyncio
import statistics
import regex as re
from concurrent.futures import ThreadPoolExecutor, as_completed
from pydantic import BaseModel
from enum import Enum
from openai import OpenAI

# Use the same configuration as the concurrent requests script
API_BASE_URL = "http://127.0.0.1:8000/v1"
MODEL_ID = "meta-llama/Llama-3.3-70B-Instruct"
API_TOKEN = os.getenv("API_TOKEN")

# For local vLLM server, API key is optional
# Use a dummy key if none is provided, as OpenAI client requires some value
client = OpenAI(
    base_url=API_BASE_URL,
    api_key=API_TOKEN or "dummy-key-for-local-server",
)

# Define the morphological analysis schema to match the concurrent requests script
class MorphologicalTerm(BaseModel):
    morphological_term: str
    analytical_category: str
    taxonomic_significance: str
    morphological_complexity: str
    output: str = ""

class ComprehensiveMorphologicalAnalysis(BaseModel):
    comprehensive_morphological_analysis: list[MorphologicalTerm]

def eval_correctness_json(expected, actual):
    """Validate JSON generated by the LLM using regex extraction and parsing."""
    # extract json string from string using regex
    actual = actual.replace("\n", "").replace(" ", "").strip()
    try:
        actual = re.search(r"\{.*\}", actual).group()
        actual = json.loads(actual)
    except Exception:
        return False

    return True

def calculate_metrics(start_time, end_time, usage_info):
    """Calculate TTFT, TPOT, and E2EL metrics from timing and token usage data."""
    total_time = end_time - start_time
    
    # Extract token counts
    prompt_tokens = usage_info.get("prompt_tokens", 0)
    completion_tokens = usage_info.get("completion_tokens", 0)
    total_tokens = usage_info.get("total_tokens", 0)
    
    # E2EL: End-to-End Latency (same as total_time for non-streaming requests)
    e2el = total_time
    
    # TTFT: For non-streaming requests, we approximate TTFT as the total time
    # since we don't have access to when the first token was generated
    ttft = total_time
    
    # TPOT: Time per output token (excluding prompt processing)
    # For non-streaming, this is total_time / completion_tokens
    tpot = total_time / completion_tokens if completion_tokens > 0 else 0
    
    # Tokens per second metrics
    total_tokens_per_sec = total_tokens / total_time if total_time > 0 else 0
    completion_tokens_per_sec = completion_tokens / total_time if total_time > 0 else 0
    
    return {
        "total_time": total_time,
        "e2el": e2el,
        "ttft": ttft,
        "tpot": tpot,
        "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens,
        "total_tokens": total_tokens,
        "total_tokens_per_sec": total_tokens_per_sec,
        "completion_tokens_per_sec": completion_tokens_per_sec
    }

def make_single_request(messages, json_schema, request_id):
    """Make a single structured output request and return metrics."""
    start_time = time.time()
    
    try:
        # Make the request using the same format as the concurrent requests script
        completion = client.chat.completions.create(
            model=MODEL_ID,
            temperature=0.0,
            max_tokens=1000,
            stream=False,
            messages=messages * 4,
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "comprehensive_morphological_analysis",
                    "schema": json_schema,
                    "strict": True
                }
            }
        )
        
        end_time = time.time()
        
        # Extract response content and usage information
        response_content = completion.choices[0].message.content
        usage_info = completion.usage.model_dump() if completion.usage else {}
        
        # Calculate metrics
        metrics = calculate_metrics(start_time, end_time, usage_info)
        
        # Validate the JSON structure using both methods
        try:
            parsed_response = json.loads(response_content)
            analysis = ComprehensiveMorphologicalAnalysis(**parsed_response)
            pydantic_validation_success = True
            num_terms = len(analysis.comprehensive_morphological_analysis)
        except (json.JSONDecodeError, ValueError) as e:
            pydantic_validation_success = False
            num_terms = 0
        
        # Additional JSON correctness validation using the provided function
        json_correctness = eval_correctness_json(None, response_content)
        
        return {
            "request_id": request_id,
            "success": True,
            "metrics": metrics,
            "response_content": response_content,
            "pydantic_validation_success": pydantic_validation_success,
            "json_correctness": json_correctness,
            "num_terms": num_terms
        }
        
    except Exception as e:
        end_time = time.time()
        processing_time = end_time - start_time
        
        return {
            "request_id": request_id,
            "success": False,
            "error": str(e),
            "processing_time": processing_time
        }

def run_concurrent_requests(messages, json_schema, num_prompts, max_concurrent):
    """Run multiple concurrent structured output requests."""
    print(f"Starting {num_prompts} concurrent requests with max concurrency of {max_concurrent}...")
    
    results = []
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
        # Submit all requests
        future_to_id = {
            executor.submit(make_single_request, messages, json_schema, i): i 
            for i in range(num_prompts)
        }
        
        # Collect results as they complete
        for future in as_completed(future_to_id):
            request_id = future_to_id[future]
            try:
                result = future.result()
                results.append(result)
                if result["success"]:
                    print(f"✓ Request {request_id + 1}/{num_prompts} completed ({result['metrics']['total_time']:.2f}s)")
                else:
                    print(f"✗ Request {request_id + 1}/{num_prompts} failed: {result['error']}")
            except Exception as e:
                print(f"✗ Request {request_id + 1}/{num_prompts} failed with exception: {e}")
                results.append({
                    "request_id": request_id,
                    "success": False,
                    "error": str(e),
                    "processing_time": 0
                })
    
    end_time = time.time()
    total_wall_time = end_time - start_time
    
    return results, total_wall_time

def calculate_aggregate_stats(results, total_wall_time):
    """Calculate aggregate statistics from all request results."""
    successful_results = [r for r in results if r["success"]]
    failed_results = [r for r in results if not r["success"]]
    
    if not successful_results:
        return {
            "total_requests": len(results),
            "successful_requests": 0,
            "failed_requests": len(failed_results),
            "success_rate": 0.0,
            "total_wall_time": total_wall_time
        }
    
    # Extract metrics from successful requests
    total_times = [r["metrics"]["total_time"] for r in successful_results]
    e2els = [r["metrics"]["e2el"] for r in successful_results]
    ttfts = [r["metrics"]["ttft"] for r in successful_results]
    tpots = [r["metrics"]["tpot"] for r in successful_results]
    prompt_tokens = [r["metrics"]["prompt_tokens"] for r in successful_results]
    completion_tokens = [r["metrics"]["completion_tokens"] for r in successful_results]
    total_tokens = [r["metrics"]["total_tokens"] for r in successful_results]
    completion_tokens_per_sec = [r["metrics"]["completion_tokens_per_sec"] for r in successful_results]
    
    # Calculate aggregate statistics
    stats = {
        "total_requests": len(results),
        "successful_requests": len(successful_results),
        "failed_requests": len(failed_results),
        "success_rate": len(successful_results) / len(results) * 100,
        "total_wall_time": total_wall_time,
        
        # Timing statistics
        "total_time": {
            "mean": statistics.mean(total_times),
            "median": statistics.median(total_times),
            "min": min(total_times),
            "max": max(total_times),
            "stdev": statistics.stdev(total_times) if len(total_times) > 1 else 0
        },
        "e2el": {
            "mean": statistics.mean(e2els),
            "median": statistics.median(e2els),
            "min": min(e2els),
            "max": max(e2els),
            "stdev": statistics.stdev(e2els) if len(e2els) > 1 else 0
        },
        "ttft": {
            "mean": statistics.mean(ttfts),
            "median": statistics.median(ttfts),
            "min": min(ttfts),
            "max": max(ttfts),
            "stdev": statistics.stdev(ttfts) if len(ttfts) > 1 else 0
        },
        "tpot": {
            "mean": statistics.mean(tpots),
            "median": statistics.median(tpots),
            "min": min(tpots),
            "max": max(tpots),
            "stdev": statistics.stdev(tpots) if len(tpots) > 1 else 0
        },
        
        # Token statistics
        "prompt_tokens": {
            "mean": statistics.mean(prompt_tokens),
            "total": sum(prompt_tokens)
        },
        "completion_tokens": {
            "mean": statistics.mean(completion_tokens),
            "total": sum(completion_tokens)
        },
        "total_tokens": {
            "mean": statistics.mean(total_tokens),
            "total": sum(total_tokens)
        },
        
        # Throughput statistics
        "completion_tokens_per_sec": {
            "mean": statistics.mean(completion_tokens_per_sec),
            "median": statistics.median(completion_tokens_per_sec),
            "min": min(completion_tokens_per_sec),
            "max": max(completion_tokens_per_sec)
        },
        
        # Validation statistics
        "pydantic_validation_success_rate": sum(1 for r in successful_results if r["pydantic_validation_success"]) / len(successful_results) * 100 if successful_results else 0,
        "json_correctness_rate": sum(1 for r in successful_results if r["json_correctness"]) / len(successful_results) * 100 if successful_results else 0,
        "avg_terms_per_response": statistics.mean([r["num_terms"] for r in successful_results]) if successful_results else 0
    }
    
    return stats

def print_aggregate_stats(stats):
    """Print formatted aggregate statistics."""
    print("=" * 80)
    print("AGGREGATE STATISTICS")
    print("=" * 80)
    
    # Overall metrics
    print(f"Total Requests: {stats['total_requests']}")
    print(f"Successful Requests: {stats['successful_requests']}")
    print(f"Failed Requests: {stats['failed_requests']}")
    print(f"Success Rate: {stats['success_rate']:.1f}%")
    print(f"Total Wall Time: {stats['total_wall_time']:.3f}s")
    
    if stats['successful_requests'] == 0:
        print("No successful requests to analyze.")
        return
    
    print("\n" + "=" * 40)
    print("TIMING STATISTICS")
    print("=" * 40)
    
    # Total time statistics
    print(f"Total Time per Request:")
    print(f"  Mean: {stats['total_time']['mean']:.3f}s")
    print(f"  Median: {stats['total_time']['median']:.3f}s")
    print(f"  Min: {stats['total_time']['min']:.3f}s")
    print(f"  Max: {stats['total_time']['max']:.3f}s")
    print(f"  StdDev: {stats['total_time']['stdev']:.3f}s")
    
    # E2EL statistics
    print(f"\nE2EL (End-to-End Latency):")
    print(f"  Mean: {stats['e2el']['mean']:.3f}s ({stats['e2el']['mean']*1000:.1f}ms)")
    print(f"  Median: {stats['e2el']['median']:.3f}s ({stats['e2el']['median']*1000:.1f}ms)")
    print(f"  Min: {stats['e2el']['min']:.3f}s ({stats['e2el']['min']*1000:.1f}ms)")
    print(f"  Max: {stats['e2el']['max']:.3f}s ({stats['e2el']['max']*1000:.1f}ms)")
    print(f"  StdDev: {stats['e2el']['stdev']:.3f}s ({stats['e2el']['stdev']*1000:.1f}ms)")
    
    # TTFT statistics
    print(f"\nTTFT (Time To First Token):")
    print(f"  Mean: {stats['ttft']['mean']:.3f}s ({stats['ttft']['mean']*1000:.1f}ms)")
    print(f"  Median: {stats['ttft']['median']:.3f}s ({stats['ttft']['median']*1000:.1f}ms)")
    print(f"  Min: {stats['ttft']['min']:.3f}s ({stats['ttft']['min']*1000:.1f}ms)")
    print(f"  Max: {stats['ttft']['max']:.3f}s ({stats['ttft']['max']*1000:.1f}ms)")
    print(f"  StdDev: {stats['ttft']['stdev']:.3f}s ({stats['ttft']['stdev']*1000:.1f}ms)")
    
    # TPOT statistics
    print(f"\nTPOT (Time Per Output Token):")
    print(f"  Mean: {stats['tpot']['mean']:.4f}s ({stats['tpot']['mean']*1000:.2f}ms)")
    print(f"  Median: {stats['tpot']['median']:.4f}s ({stats['tpot']['median']*1000:.2f}ms)")
    print(f"  Min: {stats['tpot']['min']:.4f}s ({stats['tpot']['min']*1000:.2f}ms)")
    print(f"  Max: {stats['tpot']['max']:.4f}s ({stats['tpot']['max']*1000:.2f}ms)")
    print(f"  StdDev: {stats['tpot']['stdev']:.4f}s ({stats['tpot']['stdev']*1000:.2f}ms)")
    
    print("\n" + "=" * 40)
    print("TOKEN STATISTICS")
    print("=" * 40)
    
    print(f"Prompt Tokens:")
    print(f"  Mean per request: {stats['prompt_tokens']['mean']:.1f}")
    print(f"  Total: {stats['prompt_tokens']['total']}")
    
    print(f"\nCompletion Tokens:")
    print(f"  Mean per request: {stats['completion_tokens']['mean']:.1f}")
    print(f"  Total: {stats['completion_tokens']['total']}")
    
    print(f"\nTotal Tokens:")
    print(f"  Mean per request: {stats['total_tokens']['mean']:.1f}")
    print(f"  Total: {stats['total_tokens']['total']}")
    
    print("\n" + "=" * 40)
    print("THROUGHPUT STATISTICS")
    print("=" * 40)
    
    print(f"Completion Tokens/sec:")
    print(f"  Mean: {stats['completion_tokens_per_sec']['mean']:.2f}")
    print(f"  Median: {stats['completion_tokens_per_sec']['median']:.2f}")
    print(f"  Min: {stats['completion_tokens_per_sec']['min']:.2f}")
    print(f"  Max: {stats['completion_tokens_per_sec']['max']:.2f}")
    
    # Overall throughput
    total_completion_tokens = stats['completion_tokens']['total']
    overall_throughput = total_completion_tokens / stats['total_wall_time']
    print(f"\nOverall Throughput: {overall_throughput:.2f} completion tokens/sec")
    
    print("\n" + "=" * 40)
    print("VALIDATION STATISTICS")
    print("=" * 40)
    
    print(f"Pydantic Validation Success Rate: {stats['pydantic_validation_success_rate']:.1f}%")
    print(f"JSON Correctness Rate: {stats['json_correctness_rate']:.1f}%")
    print(f"Average Terms per Response: {stats['avg_terms_per_response']:.1f}")

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Run structured output requests with configurable concurrency",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        "--max-concurrent",
        type=int,
        default=1,
        help="Maximum number of concurrent requests"
    )
    
    parser.add_argument(
        "--num-prompts",
        type=int,
        default=1,
        help="Total number of prompts to send"
    )
    
    return parser.parse_args()

def main():
    """Main function to execute structured output requests with configurable concurrency."""
    # Parse command line arguments
    args = parse_args()
    
    print("=" * 80)
    print("Structured Outputs LLM API Request")
    print(f"API Base URL: {API_BASE_URL}")
    print(f"Model: {MODEL_ID}")
    print(f"Number of Prompts: {args.num_prompts}")
    print(f"Max Concurrent Requests: {args.max_concurrent}")
    print("=" * 80)
    
    # Load the same messages as used in the concurrent requests script
    try:
        with open("messages_prompts.json", "r") as f:
            messages = json.load(f)
    except FileNotFoundError:
        print("Error: messages_prompts.json file not found!")
        return 1
    except json.JSONDecodeError as e:
        print(f"Error parsing messages_prompts.json: {e}")
        return 1
    
    # Get the JSON schema
    json_schema = ComprehensiveMorphologicalAnalysis.model_json_schema()
    
    # Handle single request case (backward compatibility)
    if args.num_prompts == 1 and args.max_concurrent == 1:
        print("Making single structured output request...")
        result = make_single_request(messages, json_schema, 0)
        
        if result["success"]:
            print("=" * 60)
            print("REQUEST COMPLETED SUCCESSFULLY")
            print("=" * 60)
            
            metrics = result["metrics"]
            
            # Display timing metrics
            print(f"Total Processing Time: {metrics['total_time']:.3f}s")
            print(f"E2EL (End-to-End Latency): {metrics['e2el']:.3f}s ({metrics['e2el']*1000:.1f}ms)")
            print(f"TTFT (Time To First Token): {metrics['ttft']:.3f}s ({metrics['ttft']*1000:.1f}ms)")
            print(f"TPOT (Time Per Output Token): {metrics['tpot']:.4f}s ({metrics['tpot']*1000:.2f}ms)")
            
            # Display token metrics
            print(f"Prompt Tokens: {metrics['prompt_tokens']}")
            print(f"Completion Tokens: {metrics['completion_tokens']}")
            print(f"Total Tokens: {metrics['total_tokens']}")
            
            # Display throughput metrics
            print(f"Total Tokens/sec: {metrics['total_tokens_per_sec']:.2f}")
            print(f"Completion Tokens/sec: {metrics['completion_tokens_per_sec']:.2f}")
            
            print("=" * 60)
            print("RESPONSE CONTENT:")
            print("=" * 60)
            print(result["response_content"])
            
            # Display validation results
            print("=" * 60)
            print("VALIDATION RESULTS:")
            print("=" * 60)
            print(f"Pydantic Validation: {'✓ PASSED' if result['pydantic_validation_success'] else '✗ FAILED'}")
            print(f"JSON Correctness: {'✓ PASSED' if result['json_correctness'] else '✗ FAILED'}")
            if result["pydantic_validation_success"]:
                print(f"Successfully parsed {result['num_terms']} morphological terms")
            print("=" * 60)
            
            return 0
        else:
            print("=" * 60)
            print("REQUEST FAILED")
            print("=" * 60)
            print(f"Processing Time: {result['processing_time']:.3f}s")
            print(f"Error: {result['error']}")
            print("=" * 60)
            return 1
    
    # Handle concurrent requests
    else:
        print(f"\nStarting concurrent execution...")
        results, total_wall_time = run_concurrent_requests(
            messages, json_schema, args.num_prompts, args.max_concurrent
        )
        
        # Calculate and display aggregate statistics
        stats = calculate_aggregate_stats(results, total_wall_time)
        print_aggregate_stats(stats)
        
        # Return success if at least some requests succeeded
        return 0 if stats['successful_requests'] > 0 else 1

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)